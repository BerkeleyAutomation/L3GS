{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yujustin/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from l3gs.data.utils.pyramid_embedding_dataloader2 import PyramidEmbeddingDataloader\n",
    "from l3gs.data.utils.dino_dataloader2 import DinoDataloader\n",
    "from l3gs.encoders.image_encoder import BaseImageEncoderConfig\n",
    "from l3gs.encoders.openclip_encoder import OpenCLIPNetworkConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils import data\n",
    "from typing import Dict, ForwardRef, Generic, List, Literal, Optional, Tuple, Type, Union, cast, get_args, get_origin\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.colormaps import apply_colormap\n",
    "import torch\n",
    "\n",
    "device = 'cuda:0'\n",
    "\"\"\"The device to run on\"\"\"\n",
    "patch_tile_size_range: Tuple[int, int] = (0.08, 0.5)\n",
    "\"\"\"The range of tile sizes to sample from for patch-based training\"\"\"\n",
    "patch_tile_size_res: int = 7\n",
    "\"\"\"The number of tile sizes to sample from for patch-based training\"\"\"\n",
    "patch_stride_scaler: float = 0.5\n",
    "\"\"\"The stride scaler for patch-based training\"\"\"\n",
    "network: BaseImageEncoderConfig = OpenCLIPNetworkConfig(device=device)\n",
    "\"\"\"specifies the vision-language network config\"\"\"\n",
    "clip_downscale_factor: int = 1\n",
    "\"\"\"The downscale factor for the clip pyramid\"\"\"\n",
    "\n",
    "dino_dataloader = DinoDataloader(\n",
    "            # image_list=images,\n",
    "            device=device,\n",
    "            cfg={\"image_shape\": [480,640]},\n",
    "            # cache_path=dino_cache_path,\n",
    "        )\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "clip_interpolator = PyramidEmbeddingDataloader(\n",
    "    device=device,\n",
    "    cfg={\n",
    "        \"tile_size_range\": list(patch_tile_size_range),\n",
    "        \"tile_size_res\": patch_tile_size_res,\n",
    "        \"stride_scaler\": patch_stride_scaler,\n",
    "        \"image_shape\": [480,640],\n",
    "    },\n",
    "    model=network.setup()\n",
    ")\n",
    "image_encoder = clip_interpolator.model\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "im_frame1 = Image.open('/home/yujustin/HDD2/rs_diff_cap/cap4png/_Color_1717622659099.65527343750000.png')\n",
    "np_frame1 = np.array(im_frame1)\n",
    "im_frame2 = Image.open('/home/yujustin/HDD2/rs_diff_cap/cap5png/_Color_1717622688820.22192382812500.png')\n",
    "np_frame2 = np.array(im_frame2)\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "f.set_figheight(15)\n",
    "f.set_figwidth(15)\n",
    "axarr[0].imshow(np_frame1)\n",
    "axarr[1].imshow(np_frame2)\n",
    "axarr[0].axis('off')\n",
    "axarr[1].axis('off')\n",
    "f.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "im_frame1 = Image.open('/home/yujustin/HDD2/grocery_store/lerf_data_2024_04_24/_Color_1717622659099.65527343750000.png')\n",
    "np_frame1 = np.array(im_frame1)\n",
    "im_frame2 = Image.open('/home/yujustin/HDD2/rs_diff_cap/cap5png/_Color_1717622688820.22192382812500.png')\n",
    "np_frame2 = np.array(im_frame2)\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "f.set_figheight(15)\n",
    "f.set_figwidth(15)\n",
    "axarr[0].imshow(np_frame1)\n",
    "axarr[1].imshow(np_frame2)\n",
    "axarr[0].axis('off')\n",
    "axarr[1].axis('off')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils.pyramid_embedding_dataloader import PyramidEmbeddingDataloader\n",
    "from encoders.image_encoder import BaseImageEncoderConfig\n",
    "from encoders.openclip_encoder import OpenCLIPNetworkConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils import data\n",
    "from typing import Dict, ForwardRef, Generic, List, Literal, Optional, Tuple, Type, Union, cast, get_args, get_origin\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.colormaps import apply_colormap\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    device = 'cuda:6'\n",
    "    \"\"\"The device to run on\"\"\"\n",
    "    patch_tile_size_range: Tuple[int, int] = (0.08, 0.5)\n",
    "    \"\"\"The range of tile sizes to sample from for patch-based training\"\"\"\n",
    "    patch_tile_size_res: int = 7\n",
    "    \"\"\"The number of tile sizes to sample from for patch-based training\"\"\"\n",
    "    patch_stride_scaler: float = 0.5\n",
    "    \"\"\"The stride scaler for patch-based training\"\"\"\n",
    "    network: BaseImageEncoderConfig = OpenCLIPNetworkConfig(device=device)\n",
    "    \"\"\"specifies the vision-language network config\"\"\"\n",
    "    clip_downscale_factor: int = 1\n",
    "    \"\"\"The downscale factor for the clip pyramid\"\"\"\n",
    "\n",
    "    clip_interpolator = PyramidEmbeddingDataloader(\n",
    "        device=device,\n",
    "        cfg={\n",
    "            \"tile_size_range\": list(patch_tile_size_range),\n",
    "            \"tile_size_res\": patch_tile_size_res,\n",
    "            \"stride_scaler\": patch_stride_scaler,\n",
    "            # \"image_shape\": [h,w],\n",
    "        },\n",
    "        model=network.setup()\n",
    "    )\n",
    "    image_encoder = clip_interpolator.model\n",
    "\n",
    "    dataset = load_dataset(\"imagenet-1k\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    ### Load human readable labels dictionary from data/labels/imagenet1k_labels.txt\n",
    "    with open(\"data/labels/imagenet1k_labels.txt\", \"r\") as f:\n",
    "        labels = f.readlines()\n",
    "    labels = [label.strip() for label in labels]\n",
    "    # List of \"930: 'ear, spike, capitulum',\" to dictionary\n",
    "    labels = {int(label.split(\":\")[0]): label.split(\":\")[1].strip(\",\").strip().strip(\"'\") for label in labels}\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    data = {}\n",
    "    for i, batch in enumerate(dataset['train']):\n",
    "\n",
    "        image = transform(batch['image'])\n",
    "        try:\n",
    "            clip_interpolator.generate_clip_interp(image)\n",
    "        except:\n",
    "            continue\n",
    "        H, W = image.shape[1:]\n",
    "\n",
    "        scale = torch.tensor(0.1).to(device)\n",
    "        scaled_height = H//clip_downscale_factor\n",
    "        scaled_width = W//clip_downscale_factor\n",
    "        # random_pixels = torch.randperm(scaled_height*scaled_width)[:int((scaled_height*scaled_height)*0.5)]\n",
    "\n",
    "        x = torch.arange(0, scaled_width*clip_downscale_factor, clip_downscale_factor).view(1, scaled_width, 1).expand(scaled_height, scaled_width, 1).to(device)\n",
    "        y = torch.arange(0, scaled_height*clip_downscale_factor, clip_downscale_factor).view(scaled_height, 1, 1).expand(scaled_height, scaled_width, 1).to(device)\n",
    "        image_idx_tensor = torch.zeros(scaled_height, scaled_width, 1).to(device)\n",
    "        positions = torch.cat((image_idx_tensor, y, x), dim=-1).view(-1, 3).to(int)\n",
    "        # positions = positions[random_pixels]\n",
    "        with torch.no_grad():\n",
    "            # data[\"clip\"], data[\"clip_scale\"] = clip_interpolator(positions, scale)[0], clip_interpolator(positions, scale)[1]\n",
    "            data[\"clip\"] = clip_interpolator(positions)[0]\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        positive = labels[batch[\"label\"]].split(\", \")\n",
    "        # import pdb; pdb.set_trace()\n",
    "        image_encoder.set_positives(positive)\n",
    "        probs = image_encoder.get_relevancy(data[\"clip\"].view(-1, image_encoder.embedding_dim), 0)\n",
    "        color = apply_colormap(probs[..., 0:1])\n",
    "        color = color.reshape([H,W,3])\n",
    "        # Show image and heatmap side by side\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(image.permute(1,2,0))\n",
    "        ax[1].imshow(color.cpu().numpy())\n",
    "        fig.suptitle(positive)\n",
    "        plt.savefig(f\"test_clip_interp_{i}_{positive}.png\")\n",
    "\n",
    "        if i == 100:\n",
    "            break\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
